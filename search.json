[
  {
    "objectID": "architecture.html",
    "href": "architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "NL-Cube is designed with a modular, clean architecture that prioritizes performance, flexibility, and offline-first operation. This document provides an overview of the system architecture, component interactions, and technical design decisions.\n\n\nNL-Cube transforms CSV and Parquet files into queryable data using natural language. The system is built around these core components:\n\nWeb Server: An Axum-based web server providing both a REST API and web UI\nDatabase Layer: DuckDB embedded database for high-performance analytics\nLLM Integration: Support for local and remote language models\nFile Ingestion: CSV and Parquet file processors\nVisualization: FINOS Perspective integration for data exploration\n\n\n\n\n\n\n\nNL-Cube_architecture.drawio.svg\n\n\n\n\n\n\n\nThe web server is built using the Axum framework, a high-performance, async Rust web framework. It provides:\n\nREST API endpoints for data operations\nWeb interface for user interaction\nStatic file serving\nMultipart file uploads\nServer-sent events for long-running operations\n\nThe web server is implemented in src/web/ with these key components:\n\nRoutes: Defined in src/web/routes.rs, mapping URLs to handlers\nHandlers: API logic in src/web/handlers/api.rs and UI in src/web/handlers/ui.rs\nState: Application state management in src/web/state.rs\n\n\n\n\nNL-Cube uses DuckDB as its embedded database engine, chosen for:\n\nColumn-oriented storage for analytical queries\nNative support for Parquet and CSV files\nHigh-performance SQL execution\nLow memory footprint\nZero-configuration deployment\n\nEach subject (data domain) gets its own DuckDB database file for isolation and organization. The database layer includes:\n\nMulti-DB Manager: Manages connections to multiple subject databases\nConnection Pool: Provides efficient connection management\nSchema Manager: Tracks database schemas for LLM context\n\n\n\n\nNL-Cube supports multiple LLM backends through a flexible provider architecture:\n\nOllama Integration: Uses Ollama for local model hosting\nRemote API: Supports cloud-based LLM APIs\n\nThe LLM integration translates natural language questions into SQL queries using:\n\nSchema extraction from the database\nContext-aware prompting\nSQL generation\nResult validation and execution\n\n\n\n\nThe ingestion system processes data files into queryable tables:\n\nCSV Ingestor: Handles comma-separated value files\nParquet Ingestor: Processes Apache Parquet files\nSchema Inference: Automatically detects column types and constraints\n\n\n\n\nFor data exploration, NL-Cube integrates the FINOS Perspective library:\n\nInteractive Pivoting: Slice and dice query results\nMultiple Views: Table, charts, and pivot views\nClient-side Filtering: Further refine results after queries\nExport Capabilities: Save results in various formats\n\n\n\n\n\n\nUser uploads CSV/Parquet files to a subject folder\nFiles are processed by the appropriate ingestor:\n\nSchema is inferred\nData is loaded into DuckDB tables\nMetadata is updated\n\nUser submits natural language query\nLLM converts query to SQL:\n\nSchema context is provided\nSQL query is generated\nQuery is validated\n\nQuery executes against DuckDB\nResults are returned as Arrow data\nPerspective visualizes the results\n\n\n\n\n\n\nRust was chosen for NL-Cube because it offers:\n\nMemory safety without garbage collection\nHigh performance for data processing\nExcellent concurrency model\nStrong type system to prevent bugs\nGood ecosystem of libraries\n\n\n\n\nDuckDB provides:\n\nColumn-oriented storage for analytics\nEmbeddable database without separate server\nNative vectorized execution\nHigh-performance for analytical queries\nDirect support for CSV and Parquet\n\n\n\n\nAxum was chosen because:\n\nBuilt on top of Tokio for async I/O\nTower middleware ecosystem\nType-safe routing\nModern, ergonomic API\nExcellent performance\n\n\n\n\nPerspective provides:\n\nHigh-performance WebAssembly-based visualization\nInteractive data exploration\nMultiple visualization types\nSeamless integration with Arrow data format\n\n\n\n\n\nNL-Cube is designed for performance with:\n\nAsync Processing: Non-blocking I/O throughout the stack\nConnection Pooling: Efficient database connection management\nArrow Data Format: Zero-copy data interchange\nMulti-threading: Parallel processing where appropriate\nStatic File Embedding: Fast access to UI resources\n\n\n\n\n\nIsolation: Each subject has its own database file\nInput Validation: All user inputs are validated\nParameterized Queries: SQL injection prevention\nFile Type Validation: Only approved file formats are accepted\n\n\n\n\nNL-Cube can be deployed in various ways:\n\nStandalone Binary: Single-file distribution\nDocker Container: Isolated, reproducible environment\nDesktop Application: Using web technologies in a desktop wrapper\n\n\n\n\nPlanned architectural enhancements include:\n\nData Sources: Use DuckDB imports for additional data sources\nAdditional LLMs: Allow user to provision different LLMs via Rig\n\n\n\n\nThe codebase is organized into these main directories:\n\nsrc/: Rust source code\n\nconfig.rs: Configuration management\ndb/: Database connection and schema management\ningest/: File ingestion (CSV, Parquet)\nllm/: Language model integration\nweb/: Web server and API\n\nstatic/: Frontend assets\n\njs/: JavaScript modules\ncss/: Styling\nindex.html: Main application page\n\ndocs/: Documentation sources",
    "crumbs": [
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#system-overview",
    "href": "architecture.html#system-overview",
    "title": "Architecture",
    "section": "",
    "text": "NL-Cube transforms CSV and Parquet files into queryable data using natural language. The system is built around these core components:\n\nWeb Server: An Axum-based web server providing both a REST API and web UI\nDatabase Layer: DuckDB embedded database for high-performance analytics\nLLM Integration: Support for local and remote language models\nFile Ingestion: CSV and Parquet file processors\nVisualization: FINOS Perspective integration for data exploration",
    "crumbs": [
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#architecture-diagram",
    "href": "architecture.html#architecture-diagram",
    "title": "Architecture",
    "section": "",
    "text": "NL-Cube_architecture.drawio.svg",
    "crumbs": [
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#component-details",
    "href": "architecture.html#component-details",
    "title": "Architecture",
    "section": "",
    "text": "The web server is built using the Axum framework, a high-performance, async Rust web framework. It provides:\n\nREST API endpoints for data operations\nWeb interface for user interaction\nStatic file serving\nMultipart file uploads\nServer-sent events for long-running operations\n\nThe web server is implemented in src/web/ with these key components:\n\nRoutes: Defined in src/web/routes.rs, mapping URLs to handlers\nHandlers: API logic in src/web/handlers/api.rs and UI in src/web/handlers/ui.rs\nState: Application state management in src/web/state.rs\n\n\n\n\nNL-Cube uses DuckDB as its embedded database engine, chosen for:\n\nColumn-oriented storage for analytical queries\nNative support for Parquet and CSV files\nHigh-performance SQL execution\nLow memory footprint\nZero-configuration deployment\n\nEach subject (data domain) gets its own DuckDB database file for isolation and organization. The database layer includes:\n\nMulti-DB Manager: Manages connections to multiple subject databases\nConnection Pool: Provides efficient connection management\nSchema Manager: Tracks database schemas for LLM context\n\n\n\n\nNL-Cube supports multiple LLM backends through a flexible provider architecture:\n\nOllama Integration: Uses Ollama for local model hosting\nRemote API: Supports cloud-based LLM APIs\n\nThe LLM integration translates natural language questions into SQL queries using:\n\nSchema extraction from the database\nContext-aware prompting\nSQL generation\nResult validation and execution\n\n\n\n\nThe ingestion system processes data files into queryable tables:\n\nCSV Ingestor: Handles comma-separated value files\nParquet Ingestor: Processes Apache Parquet files\nSchema Inference: Automatically detects column types and constraints\n\n\n\n\nFor data exploration, NL-Cube integrates the FINOS Perspective library:\n\nInteractive Pivoting: Slice and dice query results\nMultiple Views: Table, charts, and pivot views\nClient-side Filtering: Further refine results after queries\nExport Capabilities: Save results in various formats",
    "crumbs": [
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#data-flow",
    "href": "architecture.html#data-flow",
    "title": "Architecture",
    "section": "",
    "text": "User uploads CSV/Parquet files to a subject folder\nFiles are processed by the appropriate ingestor:\n\nSchema is inferred\nData is loaded into DuckDB tables\nMetadata is updated\n\nUser submits natural language query\nLLM converts query to SQL:\n\nSchema context is provided\nSQL query is generated\nQuery is validated\n\nQuery executes against DuckDB\nResults are returned as Arrow data\nPerspective visualizes the results",
    "crumbs": [
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#design-decisions",
    "href": "architecture.html#design-decisions",
    "title": "Architecture",
    "section": "",
    "text": "Rust was chosen for NL-Cube because it offers:\n\nMemory safety without garbage collection\nHigh performance for data processing\nExcellent concurrency model\nStrong type system to prevent bugs\nGood ecosystem of libraries\n\n\n\n\nDuckDB provides:\n\nColumn-oriented storage for analytics\nEmbeddable database without separate server\nNative vectorized execution\nHigh-performance for analytical queries\nDirect support for CSV and Parquet\n\n\n\n\nAxum was chosen because:\n\nBuilt on top of Tokio for async I/O\nTower middleware ecosystem\nType-safe routing\nModern, ergonomic API\nExcellent performance\n\n\n\n\nPerspective provides:\n\nHigh-performance WebAssembly-based visualization\nInteractive data exploration\nMultiple visualization types\nSeamless integration with Arrow data format",
    "crumbs": [
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#performance-considerations",
    "href": "architecture.html#performance-considerations",
    "title": "Architecture",
    "section": "",
    "text": "NL-Cube is designed for performance with:\n\nAsync Processing: Non-blocking I/O throughout the stack\nConnection Pooling: Efficient database connection management\nArrow Data Format: Zero-copy data interchange\nMulti-threading: Parallel processing where appropriate\nStatic File Embedding: Fast access to UI resources",
    "crumbs": [
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#security-model",
    "href": "architecture.html#security-model",
    "title": "Architecture",
    "section": "",
    "text": "Isolation: Each subject has its own database file\nInput Validation: All user inputs are validated\nParameterized Queries: SQL injection prevention\nFile Type Validation: Only approved file formats are accepted",
    "crumbs": [
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#deployment-options",
    "href": "architecture.html#deployment-options",
    "title": "Architecture",
    "section": "",
    "text": "NL-Cube can be deployed in various ways:\n\nStandalone Binary: Single-file distribution\nDocker Container: Isolated, reproducible environment\nDesktop Application: Using web technologies in a desktop wrapper",
    "crumbs": [
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#future-architecture-directions",
    "href": "architecture.html#future-architecture-directions",
    "title": "Architecture",
    "section": "",
    "text": "Planned architectural enhancements include:\n\nData Sources: Use DuckDB imports for additional data sources\nAdditional LLMs: Allow user to provision different LLMs via Rig",
    "crumbs": [
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#code-organization",
    "href": "architecture.html#code-organization",
    "title": "Architecture",
    "section": "",
    "text": "The codebase is organized into these main directories:\n\nsrc/: Rust source code\n\nconfig.rs: Configuration management\ndb/: Database connection and schema management\ningest/: File ingestion (CSV, Parquet)\nllm/: Language model integration\nweb/: Web server and API\n\nstatic/: Frontend assets\n\njs/: JavaScript modules\ncss/: Styling\nindex.html: Main application page\n\ndocs/: Documentation sources",
    "crumbs": [
      "Architecture"
    ]
  },
  {
    "objectID": "developer_guide.html",
    "href": "developer_guide.html",
    "title": "Developer Guide",
    "section": "",
    "text": "This guide provides information for developers who want to contribute to or extend NL-Cube. It covers project setup, code organization, and contribution guidelines.\n\n\n\n\n\nRust (1.84.0 or later, 2024 edition)\nGit\nDuckDB\nOllama (optional, for local LLM testing)\n\n\n\n\n\nClone the repository\n\ngit clone https://github.com/joefrost01/nl-cube.git\ncd nl-cube\n\nInstall Rust dependencies\n\nThe project uses Cargo for dependency management. All dependencies are specified in Cargo.toml.\n\nInstall local LLM (optional)\n\nFor local LLM testing, install Ollama and pull a SQL-focused model:\n# Install Ollama from https://ollama.ai/download\nollama pull sqlcoder\n\nConfigure the application\n\nCreate a config.toml file in the project root:\ndata_dir = \"data\"\n\n[database]\nconnection_string = \"nl-cube.db\"\npool_size = 5\n\n[web]\nhost = \"127.0.0.1\"\nport = 3000\nstatic_dir = \"static\"\n\n[llm]\nbackend = \"ollama\"\nmodel = \"sqlcoder\"\napi_url = \"http://localhost:11434/api/generate\"\n\n\n\nFor development:\ncargo run\nFor production build:\ncargo build --release\n\n\n\ncargo test\n\n\n\n\nNL-Cube is organized into several key modules:\n\n\nnl-cube/\n├── src/                 # Rust source code\n│   ├── config.rs        # Configuration management\n│   ├── db/              # Database connection and schema management\n│   ├── ingest/          # File ingestion (CSV, Parquet)\n│   ├── llm/             # Language model integration\n│   ├── util/            # Utility functions\n│   ├── web/             # Web server and API\n│   └── main.rs          # Application entry point\n├── static/              # Frontend assets\n│   ├── css/             # Stylesheets\n│   ├── js/              # JavaScript modules\n│   └── index.html       # Main application page\n├── docs/                # Documentation (Quarto)\n├── templates/           # HTML templates\n├── Cargo.toml           # Rust dependencies\n├── config.toml          # Configuration file\n└── README.md            # Project overview\n\n\n\n\n\nHandles parsing configuration from files and command-line arguments:\npub struct AppConfig {\n    pub database: DatabaseConfig,\n    pub web: WebConfig,\n    pub llm: LlmConfig,\n    pub data_dir: String,\n}\n\n\n\n\ndb_pool.rs: Connection pool management\nmulti_db_pool.rs: Multiple database support\nschema_manager.rs: Schema tracking and cache\n\n\n\n\n\ncsv.rs: CSV file processor\nparquet.rs: Parquet file processor\nschema.rs: Schema definition types\n\n\n\n\n\nmodels.rs: Data structures for LLM interactions\nproviders/: LLM backend implementations\n\nollama.rs: Ollama integration\nremote.rs: Remote API integration\n\n\n\n\n\n\nhandlers/: API and UI request handlers\nroutes.rs: URL routing\nstate.rs: Application state management\nstatic_files.rs: Static file serving\ntemplates.rs: Template rendering\n\n\n\n\n\n\nHTML: Basic structure in static/index.html\nCSS: Styling in static/css/nlcube.css\nJavaScript:\n\nnlcube.js: Main application logic\nperspective-utils.js: Visualization handling\nquery-utils.js: Query management\nupload-utils.js: File upload management\nreports-utils.js: Saved reports handling\n\n\n\n\n\n\n\n\nThe central state container that holds shared resources:\npub struct AppState {\n    pub config: AppConfig,\n    pub db_pool: Pool&lt;DuckDBConnectionManager&gt;,\n    pub llm_manager: Arc&lt;Mutex&lt;LlmManager&gt;&gt;,\n    pub data_dir: PathBuf,\n    pub subjects: RwLock&lt;Vec&lt;String&gt;&gt;,\n    pub startup_time: chrono::DateTime&lt;chrono::Utc&gt;,\n    pub current_subject: RwLock&lt;Option&lt;String&gt;&gt;,\n    pub schema_manager: SchemaManager,\n}\n\n\n\nManages language model interactions through the SqlGenerator trait:\n#[async_trait]\npub trait SqlGenerator: Send + Sync {\n    async fn generate_sql(&self, question: &str, schema: &str) -&gt; Result&lt;String, LlmError&gt;;\n}\n\n\n\nHandles file ingestion through the FileIngestor trait:\npub trait FileIngestor: Send + Sync {\n    fn ingest(\n        &self,\n        path: &Path,\n        table_name: &str,\n        subject: &str,\n    ) -&gt; Result&lt;schema::TableSchema, IngestError&gt;;\n}\n\n\n\nBuilt using Axum, the web server provides both API endpoints and UI routes:\npub fn ui_routes() -&gt; Router&lt;Arc&lt;AppState&gt;&gt; {\n    Router::new()\n        .route(\"/\", get(handlers::ui::index_handler))\n        .route(\"/static/{*path}\", get(static_handler))\n}\n\npub fn api_routes() -&gt; Router&lt;Arc&lt;AppState&gt;&gt; {\n    Router::new()\n        .nest(\n            \"/api\",\n            Router::new()\n                // Query endpoints\n                .route(\"/query\", post(handlers::api::execute_query))\n                .route(\"/nl-query\", post(sync_nl_query_handler))\n                // ... other routes\n        )\n}\n\n\n\nThe SchemaManager tracks database schemas for LLM context:\npub struct SchemaManager {\n    schema_cache: RwLock&lt;HashMap&lt;String, Vec&lt;String&gt;&gt;&gt;,\n    last_refresh: RwLock&lt;chrono::DateTime&lt;chrono::Utc&gt;&gt;,\n    data_dir: PathBuf,\n}\n\n\n\n\nNL-Cube uses several key design patterns:\n\n\nDatabase interactions are encapsulated behind traits and managers:\npub trait SqlGenerator: Send + Sync {\n    async fn generate_sql(&self, question: &str, schema: &str) -&gt; Result&lt;String, LlmError&gt;;\n}\n\n\n\nComponents receive their dependencies through constructors:\npub fn new_with_multi_db(\n    config: AppConfig,\n    db_pool: Pool&lt;DuckDBConnectionManager&gt;,\n    multi_db_manager: Arc&lt;MultiDbConnectionManager&gt;,\n    llm_manager: LlmManager,\n    data_dir: PathBuf,\n) -&gt; Self { ... }\n\n\n\nInterfaces are defined as traits, allowing for multiple implementations:\npub trait FileIngestor: Send + Sync { ... }\n\npub struct CsvIngestor { ... }\npub struct ParquetIngestor { ... }\n\n\n\nAsynchronous tasks with message passing for concurrent operations:\nlet (tx, rx) = oneshot::channel();\ntokio::task::spawn_blocking(move || {\n    // Task execution\n    let _ = tx.send(result);\n});\n// Wait for result\nmatch rx.await { ... }\n\n\n\n\n\n\n\nCreate a new struct that implements the FileIngestor trait\nAdd the ingestor to IngestManager\nUpdate file type detection in the upload handler\n\nExample:\npub struct JsonIngestor { ... }\n\nimpl FileIngestor for JsonIngestor {\n    fn ingest(\n        &self,\n        path: &Path,\n        table_name: &str,\n        subject: &str,\n    ) -&gt; Result&lt;TableSchema, IngestError&gt; {\n        // Implementation\n    }\n}\n\n\n\n\nCreate a new struct that implements the SqlGenerator trait\nAdd the provider to LlmManager\nUpdate the configuration schema\n\nExample:\npub struct CustomLlmProvider { ... }\n\n#[async_trait]\nimpl SqlGenerator for CustomLlmProvider {\n    async fn generate_sql(&self, question: &str, schema: &str) -&gt; Result&lt;String, LlmError&gt; {\n        // Implementation\n    }\n}\n\n\n\n\nAdd new route definitions in src/web/routes.rs\nCreate handler functions in src/web/handlers/api.rs\nUpdate the API documentation\n\nExample:\n// In routes.rs\n.route(\"/api/custom\", post(handlers::api::custom_handler))\n\n// In handlers/api.rs\npub async fn custom_handler(\n    State(app_state): State&lt;Arc&lt;AppState&gt;&gt;,\n    Json(payload): Json&lt;CustomRequest&gt;,\n) -&gt; Result&lt;Json&lt;CustomResponse&gt;, (StatusCode, String)&gt; {\n    // Implementation\n}\n\n\n\n\n\n\n\nFork the repository and create a feature branch\nMake your changes with appropriate tests\nEnsure all tests pass with cargo test\nFormat your code with cargo fmt\nCheck for linting issues with cargo clippy\nSubmit a pull request with a clear description of changes\n\n\n\n\n\nFollow the Rust API Guidelines\nUse async/await consistently for asynchronous code\nAdd doc comments to public interfaces\nInclude error handling with custom error types\nFormat code with rustfmt\nUse clippy to catch common mistakes\n\n\n\n\n\nUpdate documentation for API changes\nAdd doc comments for public functions\nInclude examples where appropriate\nUpdate the changelog for significant changes\n\n\n\n\n\nAdd unit tests for new functionality\nInclude integration tests for API endpoints\nTest with different configurations\nVerify performance for data-intensive operations\n\n\n\n\n\n\n\nDatabase connection errors: - Check the database file path in configuration - Verify DuckDB is installed and the correct version - Increase the connection pool size if needed\nLLM integration issues: - Verify Ollama is running (curl http://localhost:11434/api/version) - Check if the model is available (ollama list) - Review the prompt template for errors\nBuild errors: - Update Rust to the latest version (rustup update) - Clear cargo cache (cargo clean) - Check for incompatible dependency versions\n\n\n\n\nFor performance issues, use these tools:\n\nTokio Console: Monitor async tasks\nFlamegraph: Visualize CPU usage\nDHAT: Analyze heap allocations\n\nExample flamegraph generation:\ncargo install flamegraph\nCARGO_PROFILE_RELEASE_DEBUG=true cargo flamegraph --bin nl-cube\n\n\n\n\nAxum Documentation\nTokio Documentation\nDuckDB Documentation\nRust Async Book",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer_guide.html#development-environment-setup",
    "href": "developer_guide.html#development-environment-setup",
    "title": "Developer Guide",
    "section": "",
    "text": "Rust (1.84.0 or later, 2024 edition)\nGit\nDuckDB\nOllama (optional, for local LLM testing)\n\n\n\n\n\nClone the repository\n\ngit clone https://github.com/joefrost01/nl-cube.git\ncd nl-cube\n\nInstall Rust dependencies\n\nThe project uses Cargo for dependency management. All dependencies are specified in Cargo.toml.\n\nInstall local LLM (optional)\n\nFor local LLM testing, install Ollama and pull a SQL-focused model:\n# Install Ollama from https://ollama.ai/download\nollama pull sqlcoder\n\nConfigure the application\n\nCreate a config.toml file in the project root:\ndata_dir = \"data\"\n\n[database]\nconnection_string = \"nl-cube.db\"\npool_size = 5\n\n[web]\nhost = \"127.0.0.1\"\nport = 3000\nstatic_dir = \"static\"\n\n[llm]\nbackend = \"ollama\"\nmodel = \"sqlcoder\"\napi_url = \"http://localhost:11434/api/generate\"\n\n\n\nFor development:\ncargo run\nFor production build:\ncargo build --release\n\n\n\ncargo test",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer_guide.html#code-organization",
    "href": "developer_guide.html#code-organization",
    "title": "Developer Guide",
    "section": "",
    "text": "NL-Cube is organized into several key modules:\n\n\nnl-cube/\n├── src/                 # Rust source code\n│   ├── config.rs        # Configuration management\n│   ├── db/              # Database connection and schema management\n│   ├── ingest/          # File ingestion (CSV, Parquet)\n│   ├── llm/             # Language model integration\n│   ├── util/            # Utility functions\n│   ├── web/             # Web server and API\n│   └── main.rs          # Application entry point\n├── static/              # Frontend assets\n│   ├── css/             # Stylesheets\n│   ├── js/              # JavaScript modules\n│   └── index.html       # Main application page\n├── docs/                # Documentation (Quarto)\n├── templates/           # HTML templates\n├── Cargo.toml           # Rust dependencies\n├── config.toml          # Configuration file\n└── README.md            # Project overview\n\n\n\n\n\nHandles parsing configuration from files and command-line arguments:\npub struct AppConfig {\n    pub database: DatabaseConfig,\n    pub web: WebConfig,\n    pub llm: LlmConfig,\n    pub data_dir: String,\n}\n\n\n\n\ndb_pool.rs: Connection pool management\nmulti_db_pool.rs: Multiple database support\nschema_manager.rs: Schema tracking and cache\n\n\n\n\n\ncsv.rs: CSV file processor\nparquet.rs: Parquet file processor\nschema.rs: Schema definition types\n\n\n\n\n\nmodels.rs: Data structures for LLM interactions\nproviders/: LLM backend implementations\n\nollama.rs: Ollama integration\nremote.rs: Remote API integration\n\n\n\n\n\n\nhandlers/: API and UI request handlers\nroutes.rs: URL routing\nstate.rs: Application state management\nstatic_files.rs: Static file serving\ntemplates.rs: Template rendering\n\n\n\n\n\n\nHTML: Basic structure in static/index.html\nCSS: Styling in static/css/nlcube.css\nJavaScript:\n\nnlcube.js: Main application logic\nperspective-utils.js: Visualization handling\nquery-utils.js: Query management\nupload-utils.js: File upload management\nreports-utils.js: Saved reports handling",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer_guide.html#key-components",
    "href": "developer_guide.html#key-components",
    "title": "Developer Guide",
    "section": "",
    "text": "The central state container that holds shared resources:\npub struct AppState {\n    pub config: AppConfig,\n    pub db_pool: Pool&lt;DuckDBConnectionManager&gt;,\n    pub llm_manager: Arc&lt;Mutex&lt;LlmManager&gt;&gt;,\n    pub data_dir: PathBuf,\n    pub subjects: RwLock&lt;Vec&lt;String&gt;&gt;,\n    pub startup_time: chrono::DateTime&lt;chrono::Utc&gt;,\n    pub current_subject: RwLock&lt;Option&lt;String&gt;&gt;,\n    pub schema_manager: SchemaManager,\n}\n\n\n\nManages language model interactions through the SqlGenerator trait:\n#[async_trait]\npub trait SqlGenerator: Send + Sync {\n    async fn generate_sql(&self, question: &str, schema: &str) -&gt; Result&lt;String, LlmError&gt;;\n}\n\n\n\nHandles file ingestion through the FileIngestor trait:\npub trait FileIngestor: Send + Sync {\n    fn ingest(\n        &self,\n        path: &Path,\n        table_name: &str,\n        subject: &str,\n    ) -&gt; Result&lt;schema::TableSchema, IngestError&gt;;\n}\n\n\n\nBuilt using Axum, the web server provides both API endpoints and UI routes:\npub fn ui_routes() -&gt; Router&lt;Arc&lt;AppState&gt;&gt; {\n    Router::new()\n        .route(\"/\", get(handlers::ui::index_handler))\n        .route(\"/static/{*path}\", get(static_handler))\n}\n\npub fn api_routes() -&gt; Router&lt;Arc&lt;AppState&gt;&gt; {\n    Router::new()\n        .nest(\n            \"/api\",\n            Router::new()\n                // Query endpoints\n                .route(\"/query\", post(handlers::api::execute_query))\n                .route(\"/nl-query\", post(sync_nl_query_handler))\n                // ... other routes\n        )\n}\n\n\n\nThe SchemaManager tracks database schemas for LLM context:\npub struct SchemaManager {\n    schema_cache: RwLock&lt;HashMap&lt;String, Vec&lt;String&gt;&gt;&gt;,\n    last_refresh: RwLock&lt;chrono::DateTime&lt;chrono::Utc&gt;&gt;,\n    data_dir: PathBuf,\n}",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer_guide.html#design-patterns",
    "href": "developer_guide.html#design-patterns",
    "title": "Developer Guide",
    "section": "",
    "text": "NL-Cube uses several key design patterns:\n\n\nDatabase interactions are encapsulated behind traits and managers:\npub trait SqlGenerator: Send + Sync {\n    async fn generate_sql(&self, question: &str, schema: &str) -&gt; Result&lt;String, LlmError&gt;;\n}\n\n\n\nComponents receive their dependencies through constructors:\npub fn new_with_multi_db(\n    config: AppConfig,\n    db_pool: Pool&lt;DuckDBConnectionManager&gt;,\n    multi_db_manager: Arc&lt;MultiDbConnectionManager&gt;,\n    llm_manager: LlmManager,\n    data_dir: PathBuf,\n) -&gt; Self { ... }\n\n\n\nInterfaces are defined as traits, allowing for multiple implementations:\npub trait FileIngestor: Send + Sync { ... }\n\npub struct CsvIngestor { ... }\npub struct ParquetIngestor { ... }\n\n\n\nAsynchronous tasks with message passing for concurrent operations:\nlet (tx, rx) = oneshot::channel();\ntokio::task::spawn_blocking(move || {\n    // Task execution\n    let _ = tx.send(result);\n});\n// Wait for result\nmatch rx.await { ... }",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer_guide.html#extension-points",
    "href": "developer_guide.html#extension-points",
    "title": "Developer Guide",
    "section": "",
    "text": "Create a new struct that implements the FileIngestor trait\nAdd the ingestor to IngestManager\nUpdate file type detection in the upload handler\n\nExample:\npub struct JsonIngestor { ... }\n\nimpl FileIngestor for JsonIngestor {\n    fn ingest(\n        &self,\n        path: &Path,\n        table_name: &str,\n        subject: &str,\n    ) -&gt; Result&lt;TableSchema, IngestError&gt; {\n        // Implementation\n    }\n}\n\n\n\n\nCreate a new struct that implements the SqlGenerator trait\nAdd the provider to LlmManager\nUpdate the configuration schema\n\nExample:\npub struct CustomLlmProvider { ... }\n\n#[async_trait]\nimpl SqlGenerator for CustomLlmProvider {\n    async fn generate_sql(&self, question: &str, schema: &str) -&gt; Result&lt;String, LlmError&gt; {\n        // Implementation\n    }\n}\n\n\n\n\nAdd new route definitions in src/web/routes.rs\nCreate handler functions in src/web/handlers/api.rs\nUpdate the API documentation\n\nExample:\n// In routes.rs\n.route(\"/api/custom\", post(handlers::api::custom_handler))\n\n// In handlers/api.rs\npub async fn custom_handler(\n    State(app_state): State&lt;Arc&lt;AppState&gt;&gt;,\n    Json(payload): Json&lt;CustomRequest&gt;,\n) -&gt; Result&lt;Json&lt;CustomResponse&gt;, (StatusCode, String)&gt; {\n    // Implementation\n}",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer_guide.html#contributing-guidelines",
    "href": "developer_guide.html#contributing-guidelines",
    "title": "Developer Guide",
    "section": "",
    "text": "Fork the repository and create a feature branch\nMake your changes with appropriate tests\nEnsure all tests pass with cargo test\nFormat your code with cargo fmt\nCheck for linting issues with cargo clippy\nSubmit a pull request with a clear description of changes\n\n\n\n\n\nFollow the Rust API Guidelines\nUse async/await consistently for asynchronous code\nAdd doc comments to public interfaces\nInclude error handling with custom error types\nFormat code with rustfmt\nUse clippy to catch common mistakes\n\n\n\n\n\nUpdate documentation for API changes\nAdd doc comments for public functions\nInclude examples where appropriate\nUpdate the changelog for significant changes\n\n\n\n\n\nAdd unit tests for new functionality\nInclude integration tests for API endpoints\nTest with different configurations\nVerify performance for data-intensive operations",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer_guide.html#troubleshooting-development-issues",
    "href": "developer_guide.html#troubleshooting-development-issues",
    "title": "Developer Guide",
    "section": "",
    "text": "Database connection errors: - Check the database file path in configuration - Verify DuckDB is installed and the correct version - Increase the connection pool size if needed\nLLM integration issues: - Verify Ollama is running (curl http://localhost:11434/api/version) - Check if the model is available (ollama list) - Review the prompt template for errors\nBuild errors: - Update Rust to the latest version (rustup update) - Clear cargo cache (cargo clean) - Check for incompatible dependency versions",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer_guide.html#performance-profiling",
    "href": "developer_guide.html#performance-profiling",
    "title": "Developer Guide",
    "section": "",
    "text": "For performance issues, use these tools:\n\nTokio Console: Monitor async tasks\nFlamegraph: Visualize CPU usage\nDHAT: Analyze heap allocations\n\nExample flamegraph generation:\ncargo install flamegraph\nCARGO_PROFILE_RELEASE_DEBUG=true cargo flamegraph --bin nl-cube",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "developer_guide.html#additional-resources",
    "href": "developer_guide.html#additional-resources",
    "title": "Developer Guide",
    "section": "",
    "text": "Axum Documentation\nTokio Documentation\nDuckDB Documentation\nRust Async Book",
    "crumbs": [
      "Developer Guide"
    ]
  },
  {
    "objectID": "technical_details.html",
    "href": "technical_details.html",
    "title": "Technical Details",
    "section": "",
    "text": "This document provides in-depth technical information about NL-Cube’s implementation, covering database integration, LLM functionality, and the data processing pipeline.\n\n\nNL-Cube uses DuckDB as its embedded analytics database engine, chosen for its performance and ease of deployment.\n\n\nNL-Cube implements a sophisticated multi-database connection management system:\n\nConnection Pool: Maintains a configurable pool of database connections using r2d2\n// ConnectionManager implementation\nimpl ManageConnection for DuckDBConnectionManager {\n    type Connection = Connection;\n    type Error = duckdb::Error;\n\n    fn connect(&self) -&gt; Result&lt;Self::Connection, Self::Error&gt; {\n        Connection::open(&self.connection_string)\n    }\n\n    fn is_valid(&self, conn: &mut Self::Connection) -&gt; Result&lt;(), Self::Error&gt; {\n        conn.execute(\"SELECT 1\", [])?;\n        Ok(())\n    }\n\n    fn has_broken(&self, _conn: &mut Self::Connection) -&gt; bool {\n        false\n    }\n}\nMulti-Database Support: Each subject gets its own DuckDB database file\npub struct MultiDbConnectionManager {\n    main_db_path: String,\n    data_dir: PathBuf,\n    attached_dbs: Arc&lt;Mutex&lt;HashMap&lt;String, String&gt;&gt;&gt;,\n}\nThread Safety: Blocking database operations are executed in dedicated Tokio tasks\ntokio::task::spawn_blocking(move || {\n    // Database operations that might block\n    let conn = Connection::open(&db_path_string)?;\n    // ...\n})\n\n\n\n\nThe SchemaManager component maintains metadata about database schemas:\n\nSchema Discovery: Scans subject directories for database files\nCache Management: Maintains a cached view of schemas and tables\nRefresh Mechanism: Periodically updates schema information\n\npub struct SchemaManager {\n    schema_cache: RwLock&lt;HashMap&lt;String, Vec&lt;String&gt;&gt;&gt;,\n    last_refresh: RwLock&lt;chrono::DateTime&lt;chrono::Utc&gt;&gt;,\n    data_dir: PathBuf,\n}\n\n\n\nWhen executing a natural language query:\n\nContext Gathering: Schema information is extracted from the database\nLLM Translation: Natural language is translated to SQL\nExecution: SQL is executed against the appropriate subject database\nArrow Conversion: Results are converted to Apache Arrow format\nResponse: Arrow data is returned to the client for visualization\n\n// Simplified query flow\nlet table_metadata = app_state.get_table_metadata(Some(&target_subject)).await?;\nlet raw_sql = mgr.generate_sql(&payload.question, &table_metadata).await?;\nlet conn = duckdb::Connection::open(&db_path)?;\nlet arrow_batch = stmt.query_arrow([])?;\n\n\n\n\nNL-Cube features a flexible LLM integration system that supports multiple backends through a provider architecture.\n\n\nThe SqlGenerator trait defines the interface for LLM providers:\n#[async_trait]\npub trait SqlGenerator: Send + Sync {\n    async fn generate_sql(&self, question: &str, schema: &str) -&gt; Result&lt;String, LlmError&gt;;\n}\nCurrently implemented providers include:\n\nOllama Provider: For local model execution\npub struct OllamaProvider {\n    client: reqwest::Client,\n    api_url: String,\n    model: String,\n}\nRemote Provider: For cloud-based LLM APIs\npub struct RemoteLlmProvider {\n    client: reqwest::Client,\n    api_url: String,\n    api_key: String,\n    model: String,\n}\n\n\n\n\nNL-Cube uses carefully crafted prompts to guide the LLM in generating SQL:\n\nSchema Context: Database structure is provided to the LLM\nClear Instructions: The prompt contains specific SQL generation rules\nExamples: Sample queries help the model understand the expected output\n\nExample prompt template:\n### Instructions:\nYour task is to convert a question into a SQL query for DuckDB, given a database schema.\nAdhere to these rules:\n- **Be careful with column names - they are case sensitive**\n- **Use the exact spelling of column names as provided in the schema**\n- **Deliberately go through the question and database schema word by word** to appropriately answer the question\n- **Use Table Aliases** to prevent ambiguity.\n- When creating a ratio, always cast the numerator as float\n\n### Input:\nGenerate a SQL query that answers the question `{question}`.\nThis query will run on a DuckDB database with the following tables and columns:\n\n{schema}\n\n### Expected SQL Format:\n- Use lowercase for SQL keywords (SELECT, FROM, WHERE, etc.)\n- Reference column names exactly as shown in the schema\n- Make sure to use double quotes around column names with spaces or special characters\n- End your query with a semicolon\n\n### Response:\nBased on your instructions, here is the SQL query I have generated:\n```sql\n\n\n\nAfter receiving the LLM response:\n\nSQL Extraction: The generated SQL is extracted from the response\nValidation: Basic syntax checks ensure the SQL is well-formed\nParameter Stripping: Any parameters are properly formatted\n\n\n\n\n\n\n\nNL-Cube supports ingesting data from CSV and Parquet files through a flexible ingestor architecture:\npub trait FileIngestor: Send + Sync {\n    fn ingest(\n        &self,\n        path: &Path,\n        table_name: &str,\n        subject: &str,\n    ) -&gt; Result&lt;schema::TableSchema, IngestError&gt;;\n}\nThe ingestion process for each file type:\n\nCSV Ingestion:\n\nRead sample rows to infer schema\nCreate table with appropriate column types\nUse DuckDB’s read_csv_auto for optimized loading\nVerify row count after ingestion\n\nParquet Ingestion:\n\nExtract schema from Parquet metadata\nCreate matching table structure\nUse DuckDB’s read_parquet for optimized loading\nHandle binary fields and large files\n\n\n\n\n\nNL-Cube uses a combination of techniques to infer the schema from data files:\n\nType Detection: Analyzes sample data to determine column types\nNullability: Determines if columns can contain NULL values\nConstraints: Identifies potential primary keys and constraints\n\nThe schema is represented using a type-safe model:\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum DataType {\n    Integer,\n    BigInt,\n    Double,\n    String,\n    Boolean,\n    Date,\n    Timestamp,\n    Unknown(String),\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ColumnSchema {\n    pub name: String,\n    pub data_type: DataType,\n    pub nullable: bool,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct TableSchema {\n    pub name: String,\n    pub columns: Vec&lt;ColumnSchema&gt;,\n}\n\n\n\nNL-Cube uses Apache Arrow for efficient data interchange:\n\nQuery Results: Database query results are converted to Arrow format\nStreaming: Data is streamed to the client in batches\nZero-Copy: The format enables efficient memory usage\nClient Integration: Seamlessly integrates with Perspective visualization\n\n// Convert query results to Arrow\nlet arrow_batch = stmt.query_arrow([])?;\nlet schema = arrow_batch.get_schema();\nlet record_batches = arrow_batch.collect::&lt;Vec&lt;_&gt;&gt;().to_vec();\n\n// Serialize to IPC format\nlet mut buffer = Vec::new();\nlet mut file_writer = arrow::ipc::writer::FileWriter::try_new(&mut buffer, schema.deref())?;\nfor batch in &record_batches {\n    file_writer.write(batch)?;\n}\nfile_writer.finish()?;\n\n\n\n\nNL-Cube provides a comprehensive REST API for programmatic access.\n\n\n\n\nPOST /api/query\nExecutes a raw SQL query against the selected subject database.\nRequest:\n{\n  \"query\": \"SELECT * FROM orders LIMIT 10;\"\n}\nResponse: - Content-Type: application/vnd.apache.arrow.file - Headers: - X-Total-Count: Number of rows - X-Execution-Time: Execution time in ms - X-Columns: JSON array of column names - X-Generated-SQL: The executed SQL query\n\n\n\nPOST /api/nl-query\nTranslates a natural language question to SQL and executes it.\nRequest:\n{\n  \"question\": \"What are the top 5 products by revenue?\"\n}\nResponse: - Content-Type: application/vnd.apache.arrow.file - Headers: - X-Total-Count: Number of rows - X-Execution-Time: Execution time in ms - X-Columns: JSON array of column names - X-Generated-SQL: The generated SQL query\n\n\n\nGET /api/subjects\nReturns a list of available subject databases.\nGET /api/subjects/{subject}\nReturns details about a specific subject, including tables.\nPOST /api/subjects/{subject}\nCreates a new subject database.\nPOST /api/subjects/select/{subject}\nSelects a subject as the current database context.\nDELETE /api/subjects/{subject}\nDeletes a subject database.\n\n\n\nPOST /api/upload/{subject}\nUploads files to a subject database for ingestion.\n\nContent-Type: multipart/form-data\nSupports CSV and Parquet files\n\n\n\n\nGET /api/schema\nReturns the schema definition for the current subject.\n\n\n\nGET /api/reports\nReturns a list of saved reports.\nGET /api/reports/{id}\nReturns a specific saved report.\nPOST /api/reports\nSaves a new report.\nDELETE /api/reports/{id}\nDeletes a saved report.\n\n\n\n\nNL-Cube currently uses stateless authentication. Future versions will support OAuth.\n\n\n\nAPI errors follow a consistent format:\n{\n  \"error\": \"Error message\",\n  \"status\": 400\n}\nCommon status codes: - 200: Success - 400: Bad Request (invalid parameters) - 404: Not Found (subject or resource not found) - 500: Internal Server Error\n\n\n\n\nNL-Cube includes several performance optimizations:\n\nConnection Pooling: Reuses database connections for efficiency\nAsync Processing: Non-blocking I/O for web requests\nBlocking Task Offloading: CPU-intensive tasks run in dedicated threads\nArrow Data Format: Efficient data interchange\nStatic File Embedding: UI assets are embedded in the binary\n\nCritical database operations are executed in dedicated blocking tasks:\ntokio::task::spawn_blocking(move || {\n    let rt = tokio::runtime::Handle::current();\n    let result = rt.block_on(async {\n        // Database operations\n    });\n    let _ = tx.send(result);\n});\n\n\n\n\nInput Validation: All user inputs are validated before processing\nParameterized Queries: Prevents SQL injection\nFile Type Validation: Only allows approved file formats\nResource Limits: Prevents excessive resource usage\nCross-Origin Protection: CORS headers restrict access\n\n\n\n\n\n\nMinimum recommended specifications: - 4GB RAM - 2 CPU cores - 1GB free disk space\n\n\n\nFor larger deployments: - Increase pool_size in configuration - Allocate more memory for DuckDB - Consider using a more powerful machine\n\n\n\n\nDATA_DIR: Override the data directory location\nDEV_MODE: Enable template hot-reloading (development only)",
    "crumbs": [
      "Technical Details"
    ]
  },
  {
    "objectID": "technical_details.html#duckdb-integration",
    "href": "technical_details.html#duckdb-integration",
    "title": "Technical Details",
    "section": "",
    "text": "NL-Cube uses DuckDB as its embedded analytics database engine, chosen for its performance and ease of deployment.\n\n\nNL-Cube implements a sophisticated multi-database connection management system:\n\nConnection Pool: Maintains a configurable pool of database connections using r2d2\n// ConnectionManager implementation\nimpl ManageConnection for DuckDBConnectionManager {\n    type Connection = Connection;\n    type Error = duckdb::Error;\n\n    fn connect(&self) -&gt; Result&lt;Self::Connection, Self::Error&gt; {\n        Connection::open(&self.connection_string)\n    }\n\n    fn is_valid(&self, conn: &mut Self::Connection) -&gt; Result&lt;(), Self::Error&gt; {\n        conn.execute(\"SELECT 1\", [])?;\n        Ok(())\n    }\n\n    fn has_broken(&self, _conn: &mut Self::Connection) -&gt; bool {\n        false\n    }\n}\nMulti-Database Support: Each subject gets its own DuckDB database file\npub struct MultiDbConnectionManager {\n    main_db_path: String,\n    data_dir: PathBuf,\n    attached_dbs: Arc&lt;Mutex&lt;HashMap&lt;String, String&gt;&gt;&gt;,\n}\nThread Safety: Blocking database operations are executed in dedicated Tokio tasks\ntokio::task::spawn_blocking(move || {\n    // Database operations that might block\n    let conn = Connection::open(&db_path_string)?;\n    // ...\n})\n\n\n\n\nThe SchemaManager component maintains metadata about database schemas:\n\nSchema Discovery: Scans subject directories for database files\nCache Management: Maintains a cached view of schemas and tables\nRefresh Mechanism: Periodically updates schema information\n\npub struct SchemaManager {\n    schema_cache: RwLock&lt;HashMap&lt;String, Vec&lt;String&gt;&gt;&gt;,\n    last_refresh: RwLock&lt;chrono::DateTime&lt;chrono::Utc&gt;&gt;,\n    data_dir: PathBuf,\n}\n\n\n\nWhen executing a natural language query:\n\nContext Gathering: Schema information is extracted from the database\nLLM Translation: Natural language is translated to SQL\nExecution: SQL is executed against the appropriate subject database\nArrow Conversion: Results are converted to Apache Arrow format\nResponse: Arrow data is returned to the client for visualization\n\n// Simplified query flow\nlet table_metadata = app_state.get_table_metadata(Some(&target_subject)).await?;\nlet raw_sql = mgr.generate_sql(&payload.question, &table_metadata).await?;\nlet conn = duckdb::Connection::open(&db_path)?;\nlet arrow_batch = stmt.query_arrow([])?;",
    "crumbs": [
      "Technical Details"
    ]
  },
  {
    "objectID": "technical_details.html#llm-integration",
    "href": "technical_details.html#llm-integration",
    "title": "Technical Details",
    "section": "",
    "text": "NL-Cube features a flexible LLM integration system that supports multiple backends through a provider architecture.\n\n\nThe SqlGenerator trait defines the interface for LLM providers:\n#[async_trait]\npub trait SqlGenerator: Send + Sync {\n    async fn generate_sql(&self, question: &str, schema: &str) -&gt; Result&lt;String, LlmError&gt;;\n}\nCurrently implemented providers include:\n\nOllama Provider: For local model execution\npub struct OllamaProvider {\n    client: reqwest::Client,\n    api_url: String,\n    model: String,\n}\nRemote Provider: For cloud-based LLM APIs\npub struct RemoteLlmProvider {\n    client: reqwest::Client,\n    api_url: String,\n    api_key: String,\n    model: String,\n}\n\n\n\n\nNL-Cube uses carefully crafted prompts to guide the LLM in generating SQL:\n\nSchema Context: Database structure is provided to the LLM\nClear Instructions: The prompt contains specific SQL generation rules\nExamples: Sample queries help the model understand the expected output\n\nExample prompt template:\n### Instructions:\nYour task is to convert a question into a SQL query for DuckDB, given a database schema.\nAdhere to these rules:\n- **Be careful with column names - they are case sensitive**\n- **Use the exact spelling of column names as provided in the schema**\n- **Deliberately go through the question and database schema word by word** to appropriately answer the question\n- **Use Table Aliases** to prevent ambiguity.\n- When creating a ratio, always cast the numerator as float\n\n### Input:\nGenerate a SQL query that answers the question `{question}`.\nThis query will run on a DuckDB database with the following tables and columns:\n\n{schema}\n\n### Expected SQL Format:\n- Use lowercase for SQL keywords (SELECT, FROM, WHERE, etc.)\n- Reference column names exactly as shown in the schema\n- Make sure to use double quotes around column names with spaces or special characters\n- End your query with a semicolon\n\n### Response:\nBased on your instructions, here is the SQL query I have generated:\n```sql\n\n\n\nAfter receiving the LLM response:\n\nSQL Extraction: The generated SQL is extracted from the response\nValidation: Basic syntax checks ensure the SQL is well-formed\nParameter Stripping: Any parameters are properly formatted",
    "crumbs": [
      "Technical Details"
    ]
  },
  {
    "objectID": "technical_details.html#data-processing-pipeline",
    "href": "technical_details.html#data-processing-pipeline",
    "title": "Technical Details",
    "section": "",
    "text": "NL-Cube supports ingesting data from CSV and Parquet files through a flexible ingestor architecture:\npub trait FileIngestor: Send + Sync {\n    fn ingest(\n        &self,\n        path: &Path,\n        table_name: &str,\n        subject: &str,\n    ) -&gt; Result&lt;schema::TableSchema, IngestError&gt;;\n}\nThe ingestion process for each file type:\n\nCSV Ingestion:\n\nRead sample rows to infer schema\nCreate table with appropriate column types\nUse DuckDB’s read_csv_auto for optimized loading\nVerify row count after ingestion\n\nParquet Ingestion:\n\nExtract schema from Parquet metadata\nCreate matching table structure\nUse DuckDB’s read_parquet for optimized loading\nHandle binary fields and large files\n\n\n\n\n\nNL-Cube uses a combination of techniques to infer the schema from data files:\n\nType Detection: Analyzes sample data to determine column types\nNullability: Determines if columns can contain NULL values\nConstraints: Identifies potential primary keys and constraints\n\nThe schema is represented using a type-safe model:\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum DataType {\n    Integer,\n    BigInt,\n    Double,\n    String,\n    Boolean,\n    Date,\n    Timestamp,\n    Unknown(String),\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ColumnSchema {\n    pub name: String,\n    pub data_type: DataType,\n    pub nullable: bool,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct TableSchema {\n    pub name: String,\n    pub columns: Vec&lt;ColumnSchema&gt;,\n}\n\n\n\nNL-Cube uses Apache Arrow for efficient data interchange:\n\nQuery Results: Database query results are converted to Arrow format\nStreaming: Data is streamed to the client in batches\nZero-Copy: The format enables efficient memory usage\nClient Integration: Seamlessly integrates with Perspective visualization\n\n// Convert query results to Arrow\nlet arrow_batch = stmt.query_arrow([])?;\nlet schema = arrow_batch.get_schema();\nlet record_batches = arrow_batch.collect::&lt;Vec&lt;_&gt;&gt;().to_vec();\n\n// Serialize to IPC format\nlet mut buffer = Vec::new();\nlet mut file_writer = arrow::ipc::writer::FileWriter::try_new(&mut buffer, schema.deref())?;\nfor batch in &record_batches {\n    file_writer.write(batch)?;\n}\nfile_writer.finish()?;",
    "crumbs": [
      "Technical Details"
    ]
  },
  {
    "objectID": "technical_details.html#web-api-reference",
    "href": "technical_details.html#web-api-reference",
    "title": "Technical Details",
    "section": "",
    "text": "NL-Cube provides a comprehensive REST API for programmatic access.\n\n\n\n\nPOST /api/query\nExecutes a raw SQL query against the selected subject database.\nRequest:\n{\n  \"query\": \"SELECT * FROM orders LIMIT 10;\"\n}\nResponse: - Content-Type: application/vnd.apache.arrow.file - Headers: - X-Total-Count: Number of rows - X-Execution-Time: Execution time in ms - X-Columns: JSON array of column names - X-Generated-SQL: The executed SQL query\n\n\n\nPOST /api/nl-query\nTranslates a natural language question to SQL and executes it.\nRequest:\n{\n  \"question\": \"What are the top 5 products by revenue?\"\n}\nResponse: - Content-Type: application/vnd.apache.arrow.file - Headers: - X-Total-Count: Number of rows - X-Execution-Time: Execution time in ms - X-Columns: JSON array of column names - X-Generated-SQL: The generated SQL query\n\n\n\nGET /api/subjects\nReturns a list of available subject databases.\nGET /api/subjects/{subject}\nReturns details about a specific subject, including tables.\nPOST /api/subjects/{subject}\nCreates a new subject database.\nPOST /api/subjects/select/{subject}\nSelects a subject as the current database context.\nDELETE /api/subjects/{subject}\nDeletes a subject database.\n\n\n\nPOST /api/upload/{subject}\nUploads files to a subject database for ingestion.\n\nContent-Type: multipart/form-data\nSupports CSV and Parquet files\n\n\n\n\nGET /api/schema\nReturns the schema definition for the current subject.\n\n\n\nGET /api/reports\nReturns a list of saved reports.\nGET /api/reports/{id}\nReturns a specific saved report.\nPOST /api/reports\nSaves a new report.\nDELETE /api/reports/{id}\nDeletes a saved report.\n\n\n\n\nNL-Cube currently uses stateless authentication. Future versions will support OAuth.\n\n\n\nAPI errors follow a consistent format:\n{\n  \"error\": \"Error message\",\n  \"status\": 400\n}\nCommon status codes: - 200: Success - 400: Bad Request (invalid parameters) - 404: Not Found (subject or resource not found) - 500: Internal Server Error",
    "crumbs": [
      "Technical Details"
    ]
  },
  {
    "objectID": "technical_details.html#performance-optimizations",
    "href": "technical_details.html#performance-optimizations",
    "title": "Technical Details",
    "section": "",
    "text": "NL-Cube includes several performance optimizations:\n\nConnection Pooling: Reuses database connections for efficiency\nAsync Processing: Non-blocking I/O for web requests\nBlocking Task Offloading: CPU-intensive tasks run in dedicated threads\nArrow Data Format: Efficient data interchange\nStatic File Embedding: UI assets are embedded in the binary\n\nCritical database operations are executed in dedicated blocking tasks:\ntokio::task::spawn_blocking(move || {\n    let rt = tokio::runtime::Handle::current();\n    let result = rt.block_on(async {\n        // Database operations\n    });\n    let _ = tx.send(result);\n});",
    "crumbs": [
      "Technical Details"
    ]
  },
  {
    "objectID": "technical_details.html#security-considerations",
    "href": "technical_details.html#security-considerations",
    "title": "Technical Details",
    "section": "",
    "text": "Input Validation: All user inputs are validated before processing\nParameterized Queries: Prevents SQL injection\nFile Type Validation: Only allows approved file formats\nResource Limits: Prevents excessive resource usage\nCross-Origin Protection: CORS headers restrict access",
    "crumbs": [
      "Technical Details"
    ]
  },
  {
    "objectID": "technical_details.html#deployment-considerations",
    "href": "technical_details.html#deployment-considerations",
    "title": "Technical Details",
    "section": "",
    "text": "Minimum recommended specifications: - 4GB RAM - 2 CPU cores - 1GB free disk space\n\n\n\nFor larger deployments: - Increase pool_size in configuration - Allocate more memory for DuckDB - Consider using a more powerful machine\n\n\n\n\nDATA_DIR: Override the data directory location\nDEV_MODE: Enable template hot-reloading (development only)",
    "crumbs": [
      "Technical Details"
    ]
  },
  {
    "objectID": "user_guide.html",
    "href": "user_guide.html",
    "title": "User Guide",
    "section": "",
    "text": "This guide will help you get started with NL-Cube, from installation to running complex natural language queries against your data.\n\n\n\n\nDownload the latest binary for your platform:\n# Example for Linux\ncurl -L https://github.com/joefrost01/nl-cube/releases/latest/download/nl-cube-linux-x86_64 -o nl-cube\nchmod +x nl-cube\n\n\n\nCreate a config.toml file in the same directory as the binary:\ndata_dir = \"data\"\n\n[database]\nconnection_string = \"nl-cube.db\"\npool_size = 5\n\n[web]\nhost = \"127.0.0.1\"\nport = 3000\nstatic_dir = \"static\"\n\n[llm]\nbackend = \"ollama\"\nmodel = \"sqlcoder\"\napi_url = \"http://localhost:11434/api/generate\"\n\n\n\n\nOllama (if using local models): Follow installation instructions\nSQLCoder model: ollama pull sqlcoder (or another SQL-focused model)\n\n\n\n\n\n\n\nLaunch NL-Cube with your configuration:\n./nl-cube --config config.toml\nThen open your browser to http://localhost:3000\n\n\n\n\nClick on the “Databases” dropdown in the navigation bar\nSelect “New Database”\nEnter a name (e.g., “sales”)\nClick “Create”\n\n\n\n\n\nSelect your database from the dropdown\nClick “Upload” in the database panel\nSelect CSV or Parquet files containing your data\nClick “Upload”\n\nThe files will be ingested and made available as tables in your database.\n\n\n\n\nEnter a question in the “Ask Question” box, such as:\n\n“What are the top 5 products by revenue?”\n“Show me monthly sales for 2024”\n“Compare average order value by region”\n\nClick “Run Query” or press Ctrl+Enter\nView the results in the visualization panel\n\n\n\n\n\n\n\nThe core feature of NL-Cube is the ability to query your data using natural language. Here are some example questions you can ask:\nSimple Aggregations: - “What’s the total revenue across all orders?” - “How many customers do we have in each region?”\nTime-Based Analysis: - “Show me the trend of orders by month in 2024” - “What was our revenue growth rate quarter over quarter?”\nComplex Analytics: - “What’s the return rate by product category?” - “Show me products where sales have declined for 3 consecutive months”\nComparative Analysis: - “Compare performance across regions” - “Which salespeople are performing above average?”\n\n\n\nNL-Cube uses the FINOS Perspective library to provide interactive data visualizations:\n\nChange Visualization Type:\n\nClick on the visualization dropdown to select different views:\nDatagrid (default)\nVarious chart types (bar, line, scatter, etc.)\n\nInteractive Pivoting:\n\nDrag and drop columns to rows, columns, and values areas\nDynamically rearrange your data view\n\nFiltering and Sorting:\n\nClick column headers to sort\nUse the filter icon to apply filters\n\nExport Options:\n\nExport to CSV\nCopy data to clipboard\nSave current view\n\n\n\n\n\nNL-Cube allows you to organize your data into separate databases (subjects):\n\nCreating Databases:\n\nUse the “New Database” option to create logical domains\nExample domains: “Sales”, “Marketing”, “HR”, “Finance”\n\nSwitching Between Databases:\n\nSelect the database from the dropdown\nTables will update to show data from that database\n\nDatabase Isolation:\n\nEach database has its own file storage\nQueries run against the currently selected database\n\n\n\n\n\n\n\n\nAfter executing a query that produces valuable insights:\n\nClick on “Reports” in the navigation bar\nSelect “Save Report”\nEnter a name, category, and optional description\nClick “Save”\n\nSaved reports can be accessed from the Reports dropdown for future reference.\n\n\n\nNL-Cube keeps track of your recent queries:\n\nClick on “History” in the navigation bar to see recent queries\nClick on any query to run it again\nUse the “Clear” button to reset your history\n\n\n\n\nFor users who want to see the SQL behind the natural language:\n\nToggle the “Show SQL” switch\nThe generated SQL will appear below your question\nThis is helpful for learning and debugging\n\n\n\n\n\n\n\nNo data appears after upload:\n\nCheck file format (should be valid CSV or Parquet)\nVerify file has headers and valid data\nCheck browser console for any error messages\n\nQuery returns unexpected results:\n\nTry rephrasing your question to be more specific\nToggle SQL preview to see how your question was interpreted\nExamine the SQL to identify potential misinterpretations\nMake sure that you refer to column names, the LLM won’t necessarily figure out that customer means cust_id - larger LLMs will handle this better\n\nPerformance issues with large datasets:\n\nConsider splitting very large files into smaller chunks\nAdd specific filters to your natural language query\nLimit the time range in your query when possible\n\n\n\n\n\nCheck the GitHub repository for issues and updates\nSubmit bug reports using the issue template\nContribute improvements via pull requests",
    "crumbs": [
      "User Guide"
    ]
  },
  {
    "objectID": "user_guide.html#installation",
    "href": "user_guide.html#installation",
    "title": "User Guide",
    "section": "",
    "text": "Download the latest binary for your platform:\n# Example for Linux\ncurl -L https://github.com/joefrost01/nl-cube/releases/latest/download/nl-cube-linux-x86_64 -o nl-cube\nchmod +x nl-cube\n\n\n\nCreate a config.toml file in the same directory as the binary:\ndata_dir = \"data\"\n\n[database]\nconnection_string = \"nl-cube.db\"\npool_size = 5\n\n[web]\nhost = \"127.0.0.1\"\nport = 3000\nstatic_dir = \"static\"\n\n[llm]\nbackend = \"ollama\"\nmodel = \"sqlcoder\"\napi_url = \"http://localhost:11434/api/generate\"\n\n\n\n\nOllama (if using local models): Follow installation instructions\nSQLCoder model: ollama pull sqlcoder (or another SQL-focused model)",
    "crumbs": [
      "User Guide"
    ]
  },
  {
    "objectID": "user_guide.html#getting-started",
    "href": "user_guide.html#getting-started",
    "title": "User Guide",
    "section": "",
    "text": "Launch NL-Cube with your configuration:\n./nl-cube --config config.toml\nThen open your browser to http://localhost:3000\n\n\n\n\nClick on the “Databases” dropdown in the navigation bar\nSelect “New Database”\nEnter a name (e.g., “sales”)\nClick “Create”\n\n\n\n\n\nSelect your database from the dropdown\nClick “Upload” in the database panel\nSelect CSV or Parquet files containing your data\nClick “Upload”\n\nThe files will be ingested and made available as tables in your database.\n\n\n\n\nEnter a question in the “Ask Question” box, such as:\n\n“What are the top 5 products by revenue?”\n“Show me monthly sales for 2024”\n“Compare average order value by region”\n\nClick “Run Query” or press Ctrl+Enter\nView the results in the visualization panel",
    "crumbs": [
      "User Guide"
    ]
  },
  {
    "objectID": "user_guide.html#key-features",
    "href": "user_guide.html#key-features",
    "title": "User Guide",
    "section": "",
    "text": "The core feature of NL-Cube is the ability to query your data using natural language. Here are some example questions you can ask:\nSimple Aggregations: - “What’s the total revenue across all orders?” - “How many customers do we have in each region?”\nTime-Based Analysis: - “Show me the trend of orders by month in 2024” - “What was our revenue growth rate quarter over quarter?”\nComplex Analytics: - “What’s the return rate by product category?” - “Show me products where sales have declined for 3 consecutive months”\nComparative Analysis: - “Compare performance across regions” - “Which salespeople are performing above average?”\n\n\n\nNL-Cube uses the FINOS Perspective library to provide interactive data visualizations:\n\nChange Visualization Type:\n\nClick on the visualization dropdown to select different views:\nDatagrid (default)\nVarious chart types (bar, line, scatter, etc.)\n\nInteractive Pivoting:\n\nDrag and drop columns to rows, columns, and values areas\nDynamically rearrange your data view\n\nFiltering and Sorting:\n\nClick column headers to sort\nUse the filter icon to apply filters\n\nExport Options:\n\nExport to CSV\nCopy data to clipboard\nSave current view\n\n\n\n\n\nNL-Cube allows you to organize your data into separate databases (subjects):\n\nCreating Databases:\n\nUse the “New Database” option to create logical domains\nExample domains: “Sales”, “Marketing”, “HR”, “Finance”\n\nSwitching Between Databases:\n\nSelect the database from the dropdown\nTables will update to show data from that database\n\nDatabase Isolation:\n\nEach database has its own file storage\nQueries run against the currently selected database",
    "crumbs": [
      "User Guide"
    ]
  },
  {
    "objectID": "user_guide.html#advanced-usage",
    "href": "user_guide.html#advanced-usage",
    "title": "User Guide",
    "section": "",
    "text": "After executing a query that produces valuable insights:\n\nClick on “Reports” in the navigation bar\nSelect “Save Report”\nEnter a name, category, and optional description\nClick “Save”\n\nSaved reports can be accessed from the Reports dropdown for future reference.\n\n\n\nNL-Cube keeps track of your recent queries:\n\nClick on “History” in the navigation bar to see recent queries\nClick on any query to run it again\nUse the “Clear” button to reset your history\n\n\n\n\nFor users who want to see the SQL behind the natural language:\n\nToggle the “Show SQL” switch\nThe generated SQL will appear below your question\nThis is helpful for learning and debugging",
    "crumbs": [
      "User Guide"
    ]
  },
  {
    "objectID": "user_guide.html#troubleshooting",
    "href": "user_guide.html#troubleshooting",
    "title": "User Guide",
    "section": "",
    "text": "No data appears after upload:\n\nCheck file format (should be valid CSV or Parquet)\nVerify file has headers and valid data\nCheck browser console for any error messages\n\nQuery returns unexpected results:\n\nTry rephrasing your question to be more specific\nToggle SQL preview to see how your question was interpreted\nExamine the SQL to identify potential misinterpretations\nMake sure that you refer to column names, the LLM won’t necessarily figure out that customer means cust_id - larger LLMs will handle this better\n\nPerformance issues with large datasets:\n\nConsider splitting very large files into smaller chunks\nAdd specific filters to your natural language query\nLimit the time range in your query when possible\n\n\n\n\n\nCheck the GitHub repository for issues and updates\nSubmit bug reports using the issue template\nContribute improvements via pull requests",
    "crumbs": [
      "User Guide"
    ]
  },
  {
    "objectID": "roadmap.html",
    "href": "roadmap.html",
    "title": "Roadmap",
    "section": "",
    "text": "This document outlines the planned future development of NL-Cube. The roadmap is organized into short-term, medium-term, and long-term goals to provide a clear vision of where the project is headed.\n\n\n\n\nExpand NL-Cube’s data ingestion capabilities to support all formats that DuckDB can handle:\n\nJSON Files: Support for structured JSON data files\nExcel Files: Direct import of XLSX/XLS spreadsheets\nORC Files: Support for Optimized Row Columnar format\nAvro Files: Support for Apache Avro format\nXML Files: Structured XML document ingestion\nHTTP/REST Sources: Direct import from REST APIs\nDatabase Connections: Import from other databases\n\nImplementation will leverage DuckDB’s built-in capabilities:\n-- Example of how DuckDB handles different formats\nCREATE TABLE json_table AS SELECT * FROM read_json('data.json', auto_detect=true);\nCREATE TABLE excel_table AS SELECT * FROM read_excel('data.xlsx');\nCREATE TABLE orc_table AS SELECT * FROM read_orc('data.orc');\n\n\n\n\nImproved type detection for edge cases\nBetter handling of date/time formats\nDetection of primary and foreign keys\nSmart detection of hierarchical data\nPreservation of original metadata\n\n\n\n\n\n\n\n\nIntegration with Rig crate for LLM orchestration\nUser-selectable models through configuration\nDynamically switchable LLM providers\nModel version tracking and compatibility checks\nPerformance benchmarking across models\n\nConfiguration example:\n# Different models can be configured\n[[llm.models]]\nname = \"sqlcoder-34b\"\nprovider = \"databricks\"\nmodel_id = \"databricks/dbrx-instruct\"\n\n[[llm.models]]\nname = \"arctic-sqler\"\nprovider = \"anthropic\"\nmodel_id = \"claude-3-opus-20240229\"\n\n\n\n\nUser-editable prompt templates\nDomain-specific prompt optimization\nFew-shot learning examples for specialized domains\nPrompt versioning and A/B testing\n\n\n\n\n\n\n\n\nTemplate-Based Reports: Customizable report templates\nScheduled Reports: Automated report generation\nExport Formats: PDF, Excel, HTML, and Markdown\nCollaboration: Sharing and commenting on reports\nVersioning: Track changes to reports over time\n\n\n\n\n\nExpanded chart types and visualizations\nCustom visualization themes\nInteractive dashboards\nEmbeddable report widgets\nAnnotation and markup tools\n\n\n\n\n\n\n\n\nGUI for defining relationships between tables\nAutomatic foreign key detection\nEntity-relationship diagram generation\nJoin path recommendations for queries\nReferential integrity enforcement\n\nExample relationship definition:\n{\n  \"relationships\": [\n    {\n      \"from\": {\n        \"table\": \"orders\",\n        \"column\": \"customer_id\"\n      },\n      \"to\": {\n        \"table\": \"customers\",\n        \"column\": \"id\"\n      },\n      \"type\": \"many-to-one\"\n    }\n  ]\n}\n\n\n\n\nAllow addition of data to existing tables\nSchema evolution and migration\nColumn-level metadata and descriptions\nData quality constraints\nSchema versioning\n\n\n\n\n\n\n\n\nAutomated ingestion of files from watched directories\nConfigurable processing rules based on file patterns\nError handling and notification for problematic files\nThrottling and batching for high-volume scenarios\nProcessing history and audit logs\n\nConfiguration example:\n[[watch_folders]]\npath = \"data/incoming/sales\"\nsubject = \"sales\"\npattern = \"*.csv\"\ntable_prefix = \"sales_\"\npoll_interval_seconds = 30\n\n\n\n\nIntegration with Apache Kafka and other streaming platforms\nReal-time data processing pipelines\nWindowed aggregations on streaming data\nConfigurable stream processors\nStream-to-table materialization\n\nExample Kafka configuration:\n[[streaming.sources]]\ntype = \"kafka\"\nbootstrap_servers = \"kafka1:9092,kafka2:9092\"\ntopic = \"sales_data\"\ngroup_id = \"nl-cube-consumer\"\nsubject = \"sales\"\ntable = \"real_time_sales\"\n\n\n\n\nUser-defined aggregate definitions\nIncremental aggregation updates\nTime-based and event-based windows\nDirect Perspective integration for live updates\nMaterialized view management\n\n\n\n\n\n\n\n\nSupport for OAuth 2.0 authentication flows\nIntegration with identity providers (Google, GitHub, Microsoft)\nJWT token handling\nRole-based authorization\nAPI key management for programmatic access\n\n\n\n\n\nUser account management\nPersonalized settings and preferences\nResource quotas and usage tracking\nActivity logging and audit trails\nAccess control for subjects and reports\n\n\n\n\n\n\n\n\nOption to bundle lightweight local LLMs\nOptimized models for SQL generation\nNo internet dependency for core functionality\nFine-tuning tools for domain-specific datasets\nModel switchover between local and remote as needed\n\n\n\n\n\nDomain-specific sample datasets\nExample queries and reports\nGuided tutorials using sample data\nBenchmarking datasets\nEasy data reset and refresh\n\n\n\n\n\n\n\n\nIntelligent suggestions as you type\nAuto-completion for column names and values\nQuery history integration\nContext-aware suggestions based on schema\nSemantic understanding of partial queries\n\n\n\n\n\nOffline capability\nMobile-friendly responsive design\nNative app-like experience\nPush notifications\nBackground synchronization\n\n\n\n\n\nComprehensive keyboard navigation\nCustomizable keyboard shortcuts\nCommand palette for quick actions\nBatch operations\nQuery scripting capabilities\n\n\n\n\n\n\n\n\nCentralized deployment management\nUsage analytics and monitoring\nBackup and disaster recovery\nResource governance\nHealth checks and diagnostics\n\n\n\n\n\nAPI for third-party integration\nPlugin architecture\nWebhooks for event-driven workflows\nSSO integration\nEnterprise data catalog integration\n\n\n\n\n\n\n\n\nAdditional file formats: JSON, Excel\nReport saving and management\nBasic relationship definition\nData append to existing tables\nAuto-complete for column names\n\n\n\n\n\nHot watch folder for auto-ingestion\nStreaming data support (Kafka)\nUser-defined aggregates\nOAuth security integration\nLocal LLM bundling options\n\n\n\n\n\nFull multi-user mode\nEnterprise administration\nAdvanced streaming analytics\nComprehensive plugin system\nAI-assisted data modeling\n\n\n\n\n\nThe NL-Cube roadmap is guided by user feedback and community needs. We welcome contributions and suggestions through:\n\nGitHub issues and discussions\nCommunity forums\nUser feedback surveys\nUsage analytics\n\nPriority will be given to features that:\n\nImprove core natural language query capabilities\nEnhance user experience for non-technical users\nExpand data connectivity options\nSimplify deployment and administration\n\nTo contribute to the roadmap or provide feedback, please open an issue on the GitHub repository.",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#data-import-enhancements",
    "href": "roadmap.html#data-import-enhancements",
    "title": "Roadmap",
    "section": "",
    "text": "Expand NL-Cube’s data ingestion capabilities to support all formats that DuckDB can handle:\n\nJSON Files: Support for structured JSON data files\nExcel Files: Direct import of XLSX/XLS spreadsheets\nORC Files: Support for Optimized Row Columnar format\nAvro Files: Support for Apache Avro format\nXML Files: Structured XML document ingestion\nHTTP/REST Sources: Direct import from REST APIs\nDatabase Connections: Import from other databases\n\nImplementation will leverage DuckDB’s built-in capabilities:\n-- Example of how DuckDB handles different formats\nCREATE TABLE json_table AS SELECT * FROM read_json('data.json', auto_detect=true);\nCREATE TABLE excel_table AS SELECT * FROM read_excel('data.xlsx');\nCREATE TABLE orc_table AS SELECT * FROM read_orc('data.orc');\n\n\n\n\nImproved type detection for edge cases\nBetter handling of date/time formats\nDetection of primary and foreign keys\nSmart detection of hierarchical data\nPreservation of original metadata",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#expanded-llm-support",
    "href": "roadmap.html#expanded-llm-support",
    "title": "Roadmap",
    "section": "",
    "text": "Integration with Rig crate for LLM orchestration\nUser-selectable models through configuration\nDynamically switchable LLM providers\nModel version tracking and compatibility checks\nPerformance benchmarking across models\n\nConfiguration example:\n# Different models can be configured\n[[llm.models]]\nname = \"sqlcoder-34b\"\nprovider = \"databricks\"\nmodel_id = \"databricks/dbrx-instruct\"\n\n[[llm.models]]\nname = \"arctic-sqler\"\nprovider = \"anthropic\"\nmodel_id = \"claude-3-opus-20240229\"\n\n\n\n\nUser-editable prompt templates\nDomain-specific prompt optimization\nFew-shot learning examples for specialized domains\nPrompt versioning and A/B testing",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#report-management",
    "href": "roadmap.html#report-management",
    "title": "Roadmap",
    "section": "",
    "text": "Template-Based Reports: Customizable report templates\nScheduled Reports: Automated report generation\nExport Formats: PDF, Excel, HTML, and Markdown\nCollaboration: Sharing and commenting on reports\nVersioning: Track changes to reports over time\n\n\n\n\n\nExpanded chart types and visualizations\nCustom visualization themes\nInteractive dashboards\nEmbeddable report widgets\nAnnotation and markup tools",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#advanced-data-modeling",
    "href": "roadmap.html#advanced-data-modeling",
    "title": "Roadmap",
    "section": "",
    "text": "GUI for defining relationships between tables\nAutomatic foreign key detection\nEntity-relationship diagram generation\nJoin path recommendations for queries\nReferential integrity enforcement\n\nExample relationship definition:\n{\n  \"relationships\": [\n    {\n      \"from\": {\n        \"table\": \"orders\",\n        \"column\": \"customer_id\"\n      },\n      \"to\": {\n        \"table\": \"customers\",\n        \"column\": \"id\"\n      },\n      \"type\": \"many-to-one\"\n    }\n  ]\n}\n\n\n\n\nAllow addition of data to existing tables\nSchema evolution and migration\nColumn-level metadata and descriptions\nData quality constraints\nSchema versioning",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#real-time-data-processing",
    "href": "roadmap.html#real-time-data-processing",
    "title": "Roadmap",
    "section": "",
    "text": "Automated ingestion of files from watched directories\nConfigurable processing rules based on file patterns\nError handling and notification for problematic files\nThrottling and batching for high-volume scenarios\nProcessing history and audit logs\n\nConfiguration example:\n[[watch_folders]]\npath = \"data/incoming/sales\"\nsubject = \"sales\"\npattern = \"*.csv\"\ntable_prefix = \"sales_\"\npoll_interval_seconds = 30\n\n\n\n\nIntegration with Apache Kafka and other streaming platforms\nReal-time data processing pipelines\nWindowed aggregations on streaming data\nConfigurable stream processors\nStream-to-table materialization\n\nExample Kafka configuration:\n[[streaming.sources]]\ntype = \"kafka\"\nbootstrap_servers = \"kafka1:9092,kafka2:9092\"\ntopic = \"sales_data\"\ngroup_id = \"nl-cube-consumer\"\nsubject = \"sales\"\ntable = \"real_time_sales\"\n\n\n\n\nUser-defined aggregate definitions\nIncremental aggregation updates\nTime-based and event-based windows\nDirect Perspective integration for live updates\nMaterialized view management",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#security-and-multi-user-support",
    "href": "roadmap.html#security-and-multi-user-support",
    "title": "Roadmap",
    "section": "",
    "text": "Support for OAuth 2.0 authentication flows\nIntegration with identity providers (Google, GitHub, Microsoft)\nJWT token handling\nRole-based authorization\nAPI key management for programmatic access\n\n\n\n\n\nUser account management\nPersonalized settings and preferences\nResource quotas and usage tracking\nActivity logging and audit trails\nAccess control for subjects and reports",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#local-llm-integration",
    "href": "roadmap.html#local-llm-integration",
    "title": "Roadmap",
    "section": "",
    "text": "Option to bundle lightweight local LLMs\nOptimized models for SQL generation\nNo internet dependency for core functionality\nFine-tuning tools for domain-specific datasets\nModel switchover between local and remote as needed\n\n\n\n\n\nDomain-specific sample datasets\nExample queries and reports\nGuided tutorials using sample data\nBenchmarking datasets\nEasy data reset and refresh",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#user-experience-improvements",
    "href": "roadmap.html#user-experience-improvements",
    "title": "Roadmap",
    "section": "",
    "text": "Intelligent suggestions as you type\nAuto-completion for column names and values\nQuery history integration\nContext-aware suggestions based on schema\nSemantic understanding of partial queries\n\n\n\n\n\nOffline capability\nMobile-friendly responsive design\nNative app-like experience\nPush notifications\nBackground synchronization\n\n\n\n\n\nComprehensive keyboard navigation\nCustomizable keyboard shortcuts\nCommand palette for quick actions\nBatch operations\nQuery scripting capabilities",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#enterprise-features",
    "href": "roadmap.html#enterprise-features",
    "title": "Roadmap",
    "section": "",
    "text": "Centralized deployment management\nUsage analytics and monitoring\nBackup and disaster recovery\nResource governance\nHealth checks and diagnostics\n\n\n\n\n\nAPI for third-party integration\nPlugin architecture\nWebhooks for event-driven workflows\nSSO integration\nEnterprise data catalog integration",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#development-milestones",
    "href": "roadmap.html#development-milestones",
    "title": "Roadmap",
    "section": "",
    "text": "Additional file formats: JSON, Excel\nReport saving and management\nBasic relationship definition\nData append to existing tables\nAuto-complete for column names\n\n\n\n\n\nHot watch folder for auto-ingestion\nStreaming data support (Kafka)\nUser-defined aggregates\nOAuth security integration\nLocal LLM bundling options\n\n\n\n\n\nFull multi-user mode\nEnterprise administration\nAdvanced streaming analytics\nComprehensive plugin system\nAI-assisted data modeling",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#feedback-and-prioritization",
    "href": "roadmap.html#feedback-and-prioritization",
    "title": "Roadmap",
    "section": "",
    "text": "The NL-Cube roadmap is guided by user feedback and community needs. We welcome contributions and suggestions through:\n\nGitHub issues and discussions\nCommunity forums\nUser feedback surveys\nUsage analytics\n\nPriority will be given to features that:\n\nImprove core natural language query capabilities\nEnhance user experience for non-technical users\nExpand data connectivity options\nSimplify deployment and administration\n\nTo contribute to the roadmap or provide feedback, please open an issue on the GitHub repository.",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NL-Cube",
    "section": "",
    "text": "NL-Cube is an offline-first, natural language analytics engine that transforms CSV and Parquet files into lightning-fast, pivotable data cubes powered by DuckDB — with full natural language querying built-in.\nNo cloud dependency. No setup hell. One binary. Your data, your control.\n\n\n\nNatural Language Querying\nAsk real questions (“Top 5 regions by revenue in 2025”) and get instant answers.\nFlexible Data Ingestion\nDrop CSV or Parquet files into folders to populate your own subject areas.\nLightning-Fast Execution\nQueries are translated into optimized DuckDB SQL and executed instantly.\nEmbedded UI\nAnalyze, pivot, and visualize your data using an integrated FINOS Perspective front-end.\nOffline, Secure, Private\nRuns entirely on your laptop or server. No mandatory cloud APIs. Full local control.\nModel Flexibility\nSupports multiple LLM backends (local LLMs like SQLCoder via ezllama, or cloud models via Rig).\n\n\n\n\n\nCreate Subjects\nOrganize your datasets into logical folders called “Subjects”.\nDrop Data\nAdd CSV or Parquet files. NL-Cube ingests them automatically.\nAsk Questions\nQuery your data in natural language. NL-Cube translates your question into SQL.\nExplore Results\nUse the embedded Perspective UI to slice, dice, and visualize your answers.\n\n\n\n\n\nRust for extreme performance and reliability.\nDuckDB for fast, embeddable analytics.\nSQLCoder / Rig / ezllama for local and flexible LLM-driven querying.\nAxum + HTML + JavaScript + Perspective for the lightweight UI.\n\n\n\n\nNL-Cube is designed for engineers, analysts, and builders who want:\n\nZero cloud lock-in.\nTrue local-first analytics.\nFull control over how their data is queried and presented.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#highlights",
    "href": "index.html#highlights",
    "title": "NL-Cube",
    "section": "",
    "text": "Natural Language Querying\nAsk real questions (“Top 5 regions by revenue in 2025”) and get instant answers.\nFlexible Data Ingestion\nDrop CSV or Parquet files into folders to populate your own subject areas.\nLightning-Fast Execution\nQueries are translated into optimized DuckDB SQL and executed instantly.\nEmbedded UI\nAnalyze, pivot, and visualize your data using an integrated FINOS Perspective front-end.\nOffline, Secure, Private\nRuns entirely on your laptop or server. No mandatory cloud APIs. Full local control.\nModel Flexibility\nSupports multiple LLM backends (local LLMs like SQLCoder via ezllama, or cloud models via Rig).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-it-works",
    "href": "index.html#how-it-works",
    "title": "NL-Cube",
    "section": "",
    "text": "Create Subjects\nOrganize your datasets into logical folders called “Subjects”.\nDrop Data\nAdd CSV or Parquet files. NL-Cube ingests them automatically.\nAsk Questions\nQuery your data in natural language. NL-Cube translates your question into SQL.\nExplore Results\nUse the embedded Perspective UI to slice, dice, and visualize your answers.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#key-technologies",
    "href": "index.html#key-technologies",
    "title": "NL-Cube",
    "section": "",
    "text": "Rust for extreme performance and reliability.\nDuckDB for fast, embeddable analytics.\nSQLCoder / Rig / ezllama for local and flexible LLM-driven querying.\nAxum + HTML + JavaScript + Perspective for the lightweight UI.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#philosophy",
    "href": "index.html#philosophy",
    "title": "NL-Cube",
    "section": "",
    "text": "NL-Cube is designed for engineers, analysts, and builders who want:\n\nZero cloud lock-in.\nTrue local-first analytics.\nFull control over how their data is queried and presented.",
    "crumbs": [
      "Introduction"
    ]
  }
]